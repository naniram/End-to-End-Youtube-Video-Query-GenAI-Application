{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required libraries:  \n",
    "langchain google-generativeai youtube_transcript_api faiss-cpu  \n",
    "\n",
    "Replace \"your_google_palm_api_key_here\" with your actual Google PaLM API key.  \n",
    "Run the script and enter a YouTube video URL when prompted.  \n",
    "Ask questions about the video content, and the RAG application will provide answers based on the transcript.  \n",
    "\n",
    "This script does the following:  \n",
    "  \n",
    "Fetches the transcript from the YouTube video using youtube_transcript_api.  \n",
    "Splits the transcript into smaller chunks using RecursiveCharacterTextSplitter.  \n",
    "Creates embeddings for these chunks using GooglePalmEmbeddings.  \n",
    "Stores the embeddings in a FAISS vector database for efficient retrieval.  \n",
    "Sets up a Retrieval-Augmented Generation chain using GooglePalm as the LLM and the FAISS vector store as the retriever.  \n",
    "Allows the user to ask questions about the video content, retrieving relevant information from the transcript and generating answers.  \n",
    "  \n",
    "The RAG approach enhances the LLM's responses by grounding them in the specific content of the video transcript, potentially improving accuracy and relevance.  \n",
    "Note that the effectiveness of this application depends on the quality of the transcript, the capabilities of the Google PaLM model, and the nature of the questions asked. Also, be mindful of usage limits and costs associated with the Google PaLM API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_transcript(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        return \" \".join([entry['text'] for entry in transcript])\n",
    "    except Exception as e:\n",
    "        print(f\"An error occured while fetching script: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    embeddings = GooglePalmEmbeddings()\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rag_chain(vector_store):\n",
    "    llm = GooglePalm(temperature=0.1)\n",
    "\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm = llm,\n",
    "        chain_type = \"stuff\",\n",
    "        retriever = vector_store.as_retriever(),\n",
    "        return_source_documents = True\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching transcript...\n",
      "creating vector database.....\n",
      "setting up RAG chain.....\n",
      "Ready for questions!\n",
      "('\\nAnswer:', 'A day in the life of a software engineer')\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #Get video URL\n",
    "    video_url = input('Enter the video URL: ')\n",
    "    video_id = video_url.split(\"v=\")[1]\n",
    "\n",
    "    # Get Transcript\n",
    "    print('Fetching transcript...')\n",
    "    transcript = get_youtube_transcript(video_id)\n",
    "    if not transcript:\n",
    "        return\n",
    "    \n",
    "    # create vector database\n",
    "    print(\"creating vector database.....\")\n",
    "    vector_store = create_vector_db(transcript)\n",
    "\n",
    "    # set up RAG chain\n",
    "    print(\"setting up RAG chain.....\")\n",
    "    rag_chain = setup_rag_chain(vector_store)\n",
    "\n",
    "    #Query loop\n",
    "    print(\"Ready for questions!\")\n",
    "    while True:\n",
    "        query = input(\"\\nEnter your question(or 'quit' to exit): \")\n",
    "        if query.lower == 'quit':\n",
    "            break\n",
    "\n",
    "        #Get answer\n",
    "        result = rag_chain({\"query\": query})\n",
    "        print((\"\\nAnswer:\", result['result']))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
